<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Random Walks with Tweedie: A Unified View of Score-Based Diffusion Models">
    <meta name="author" content="Chicago Y. Park,
    Michael T. McCann,
    Cristina Garcia-Cardona,
    Brendt Wohlberg,
    Ulugbek S. Kamilov">

    <title>Random Walks with Tweedie: A Unified View of Score-Based Diffusion Models</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="src/css/style.css">

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>

<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Random Walks with Tweedie: <br>A Unified View of Score-Based Diffusion Models</h2> <h2></h2>
    <h3></h3> 
            <p class="abstract"><b>IEEE Signal Processing Magazine Feature on Score-Based Diffusion Models</b></p>
    <hr>
    <p class="authors">
        <a href="https://chicagopark.github.io"> Chicago Y. Park<sup>1</sup></a>,
        <a href="https://michael-t-mccann.github.io/"> Michael T. McCann<sup>2</sup></a>,
        <a href="https://scholar.google.com/citations?user=WIxIUC4AAAAJ&hl=en"> Cristina Garcia-Cardona<sup>3</sup></a>,<br>
        <a href="https://brendt.wohlberg.net/"> Brendt Wohlberg<sup>3</sup></a>,
        <a href="https://ukmlv.github.io/"> Ulugbek S. Kamilov<sup>1</sup></a> </br>
    </p>
    <p>
        <a><sup>1</sup>Computational Imaging Group, Washington University in St.Louis</a></br>
        <a><sup>2</sup>Theoretical Division, Los Alamos National Laboratory</a></br>
        <a><sup>3</sup>Computer, Computational and Statistical Sciences Division, Los Alamos National Laboratory</a></br>
		</br>

    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://ieeexplore.ieee.org/document/11168126">Paper</a>
        <a class="btn btn-primary" href="https://arxiv.org/abs/2411.18702">Preprint</a>
        <a class="btn btn-primary" href="https://github.com/wustl-cig/randomwalk_diffusion">Code</a>      
    </div>

    
</div>



       

<div class="container">
    <div class="section">

     <div class="row align-items-center">
		<div class="col justify-content-center text-center">
        		<img src="./img/spm_logo.png" style="width:100%" alt="Banner">
            </div>
        </div>

        <br>
        <h2>Abstract</h2>
        <hr>
        <p>
            We present a concise derivation for several influential score-based diffusion models
that relies on only a few textbook results.
Diffusion models have recently emerged as powerful tools for generating realistic, synthetic signals --- particularly natural images --- and often play a role in state-of-the-art algorithms for inverse problems in image processing.
While these algorithms are often surprisingly simple,
the theory behind them is not,
and multiple complex theoretical justifications exist in the literature.
Here,
we provide a simple and largely self-contained theoretical justification
for score-based diffusion models that is targeted towards the signal processing community.
This approach leads to generic algorithmic templates for training and generating samples with diffusion models.
We show that several influential diffusion models correspond to particular choices within these templates and demonstrate that alternative, more straightforward algorithmic choices can provide comparable results.
This approach has the added benefit of enabling conditional sampling without any likelihood approximation.
        </p>
    </div>

<div class="section">
    <h2>Essential Theories in Score-Based Diffusion Models</h2>
    <hr>

    <p><b>1. MMSE Denoiser and Its Approximation via Deep Denoiser</b></p>
    <p>
		Let \( \mathbf{X} \in \mathbb{R}^d \) be a clean signal and \( \mathbf{X}_\sigma = \mathbf{X} + \sigma \mathbf{n} \) be its noisy observation with Gaussian noise \( \mathbf{n} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \).
    The minimum mean squared error (MMSE) estimator is defined as the conditional expectation:
    </p>
    <div class="math-display">
        \[ \hat{\mathbf{x}}_{\text{MMSE}}(\mathbf{x}_{\sigma}) = \mathbb{E}[\mathbf{X} \mid \mathbf{X}_\sigma = \mathbf{x}_{\sigma}] \]
    </div>
	<p>
	    A deep denoiser \( \mathsf{D}_{\theta} \), trained to minimize the mean squared error (MSE) between its output and the corresponding clean sample, approximates this MMSE estimator.
	    As the training dataset grows, the denoiser converges to the optimal MMSE solution.
	</p>


    <p><b>2. Connection Between Denoiser and Tweedie’s Formula</b></p>
    <p>
        Tweedie’s formula establishes a connection between the score function of the noisy distribution \( f_{\mathbf{X}_\sigma} \) and the MMSE denoiser. Specifically, under Gaussian noise corruption, the score can be approximated as:
    </p>
    <div class="math-display">
        \[
        \nabla \log f_{\mathbf{X}_\sigma}(\mathbf{x}_{\sigma}) \approx \frac{\mathsf{D}_{\theta}(\mathbf{x}_{\sigma}) - \mathbf{x}_{\sigma}}{\sigma^2}
        \]
    </div>
    <p>
        This identity enables the use of a learned denoiser to estimate the score function, providing a practical bridge between supervised denoising and score-based generative modeling.
    </p>

    <p><b>3. Sampling with Score Using Random Walks</b></p>
    <p>
        Langevin dynamics provide a way to sample from a distribution using only its score function. The discretized update rule is:
    </p>
    <div class="math-display">
        \[
        \mathbf{x}_{k+1} = \mathbf{x}_k + \tau_k \nabla \log f_{\mathbf{X}_{\sigma_k}}(\mathbf{x}_k) + \sqrt{2\tau_k \mathcal{T}_k} \, \mathbf{n}
        \]
    </div>
<p>
    This defines a random walk guided by the score function, with step size \( \tau_k \) and temperature \( \mathcal{T}_k \).
    By choosing these parameters appropriately, many popular sampling algorithms such as NCSN [1], DDPM [2], and Score-SDE [3] can be recovered as special cases (see Table 1 for specific parameter choices).
    Beyond replicating these algorithms, our paper demonstrates that even straightforward choices of these configurable parameters can yield high-quality samples—without strictly adhering to algorithm-specific sampling rules.
    This highlights that diffusion sampling is more flexible than often assumed and, at its core, can be interpreted as a form of stochastic gradient ascent.
</p>



    <p><b>4. Cross-Compatible Use of Different Types of Score within Random Walks</b></p>
    <p>
        The noise perturbation used in score-based models follows either a variance-exploding (VE) or variance-preserving (VP) formulation.
        In the VE case (e.g., NCSN), noise is added as:
    </p>
    <div class="math-display">
        \[ \mathbf{x}^{\text{VE}}_t = \mathbf{x} + \sigma_t \mathbf{n} \]
    </div>
    <p>
        while in the VP case (e.g., DDPM), the perturbation is:
    </p>
    <div class="math-display">
        \[ \mathbf{x}^{\text{VP}}_t = \sqrt{\bar{\alpha}_t} \left( \mathbf{x} + \sqrt{\tfrac{1 - \bar{\alpha}_t}{\bar{\alpha}_t}} \mathbf{n} \right) \]
    </div>
    <p>
        These perturbations lead to different score functions, but they can be related by a unified form:
    </p>
    <div class="math-display">
        \[
        \nabla \log f_{\mathbf{X}_{\sigma}}(\mathbf{x})
        = \nabla \log f_{\mathbf{X}^{\text{VE}}_t}(\mathbf{x})
        = \sqrt{\bar{\alpha}_t} \nabla \log f_{\mathbf{X}^{\text{VP}}_t}(\sqrt{\bar{\alpha}_t} \, \mathbf{x})
        \]
    </div>
<p>
    This compatibility enables the use of pretrained VE and VP models within a single random walk framework, enhancing the generality of our approach.
    As illustrated in Figure 2, samples generated using VE or VP scores remain visually consistent—even without using sampling algorithms specifically tailored to each case. This highlights the flexibility of our formulation and the robustness of its unified design.
</p>


<p><b>5. Solving Inverse Problems with Learned Score-based Diffusion Priors</b></p>
<p>
    In inverse problems, the goal is to recover an unknown signal \( \mathbf{x} \in \mathbb{R}^d \) from degraded measurements \( \mathbf{y} = \mathbf{A} \mathbf{x} + \mathbf{e} \), where \( \mathbf{A} \) is a known forward operator and \( \mathbf{e} \sim \mathcal{N}(0, \eta^2 \mathbf{I}) \) models Gaussian noise.
    A principled solution is to sample from the posterior distribution \( f_{\mathbf{X} \mid \mathbf{Y}}(\mathbf{x} \mid \mathbf{y}) \propto f_{\mathbf{Y} \mid \mathbf{X}}(\mathbf{y} \mid \mathbf{x}) f_{\mathbf{X}}(\mathbf{x}) \), combining a learned prior with a known likelihood.
</p>
<p>
    While many existing methods approximate the intractable gradient \( \nabla \log f_{\mathbf{Y} \mid \mathbf{X}_{\sigma_k}}(\mathbf{y} \mid \mathbf{x}_k) \), our framework bypasses this step by directly incorporating the known likelihood gradient \( \nabla \log f_{\mathbf{Y} \mid \mathbf{X}}(\mathbf{y} \mid \mathbf{x}) \).
    This yields a simple yet effective update rule:
</p>
<div class="math-display">
\[
\mathbf{x}_{k+1} = \mathbf{x}_k + \tau_k \nabla \log f_{\mathbf{X}_{\sigma_k}}(\mathbf{x}_k) + \frac{\tau_k}{\eta^2} \mathbf{A}^T ( \mathbf{y} - \mathbf{A} \mathbf{x}_k ) + \sqrt{2\tau_k \mathcal{T}_k} \, \mathbf{n}
\]

</div>
<p>
    As the noise level \( \sigma_k \to 0 \), the update rule asymptotically recovers the posterior score \( \nabla \log f_{\mathbf{X} \mid \mathbf{Y}}(\mathbf{x}) \), enabling direct posterior sampling.
</p>


</div>





    <div class="section">
		<h2>Popular Sampling Algorithms as Parameterized Random Walks</h2>
        <hr>

        <div class="row align-items-center">
			<div class="col justify-content-center text-left">
                <img src="img/template_table.png" style="width:100%" alt="Banner">
                <p><b>Table 1:</b> Proposed templates and parameter settings for common diffusion model algorithms, including the noise conditional score network (NCSN), the denoising diffusion probabilistic model (DDPM), and variance-exploding (VE) and variance-preserving (VP) diffusion models in SDE. The templates unify algorithmic configurations for sampling (specifying the noise level \(\sigma_k\), step size \(\tau_k\), and temperature parameter \(\mathcal{T}_k\;\)).
</p>
            </div>
        </div>
    

 
    </div>

    <div class="section">
        <h2>Random Walks Can Use Any Type of Score Functions</h2>
        <hr>
        <div class="row align-items-center">
			<div class="col justify-content-center text-left">
                <img src="img/randomwalks_with_any_score.png" style="width:100%" alt="Banner">
                <p> <b>Figure 2:</b> Unconditional image generation with VP and VE-score-based sequence of random walks. This figure illustrates the flexibility of our sequence of random walks framework, which enables unconditional sampling with the VE score while also being compatible with the VP score under a unified framework. This implies that our framework can use any type of score without restricting the score training scheme.
 </i> </p>
            </div>
        </div>


    </div>

    <div class="section">
        <h2>Random Walks Can Use Arbitrary Noise Level Schedule</h2>
        <hr>

     <div class="row align-items-center">
		<div class="col justify-content-center text-left">
                        		<img src="./img/figures_per_scheduling.png" style="width:100%" alt="Banner">
                <p><b>Figure 1:</b> Unconditional image generation using the proposed score-based random walk framework, which decouples training and sampling to enable flexible noise schedules, step sizes, and temperature settings.</p>
            </div>
        </div>

    <div class="section">
        <h2>Likelihood Approximation-Free Posterior Sampling with Random Walks</h2>
        <hr>
        <div class="row align-items-center">
			<div class="col justify-content-center text-left">
                <img src="img/inv_problems_sigmoid_abstract.png" style="width:100%" alt="Banner">
                <p> <b>Figure 3:</b> Conditional image sampling results with the score-based sequence of random walks with sigmoid noise scheduling. We conditionally sample four images under the same setup. This figure demonstrates that our simplified framework extends beyond unconditional image synthesis to effectively solve inverse problems.
 </i> </p>
            </div>
        </div>


    </div>
<div class="section">
    <h4>References</h4>
    <ol>
		<li>Y. Song and S. Ermon, “Generative modeling by estimating gradients of the data distribution,” in Advances in Neural Information Processing Systems, vol. 32, 2019.</li>
        <li>J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in Advances in Neural Information Processing
Systems, vol. 33, 2020, pp. 6840–6851.</li>
        <li>Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, “Score-based generative modeling through
stochastic differential equations,” in 9th International Conference on Learning Representations (ICLR), 2021.</li>
    </ol>
</div>

    
    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2411.18702"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
		@article{park2025randomwalks,
		title={Random Walks with Tweedie: A Unified View of Score-Based Diffusion Models},
		author={Park, Chicago Y.
			and McCann, Michael T.
			and Garcia-Cardona, Cristina
			and Wohlberg, Brendt
			and Kamilov, Ulugbek S.},
		journal={IEEE Signal Processing Magazine},
		volume={42},
		number={3},
		pages={40--51},
		year={2025},
		publisher={IEEE},
		doi={10.1109/MSP.2025.3590608},
		url={https://doi.org/10.1109/MSP.2025.3590608}
		}        </div>
    </div>

		
		
    <hr>

   <footer>
       <p>Acknoledgement: Web template from <a href="https://www.vincentsitzmann.com/siren/">SIREN</a></p>
   </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
